from bertopic import BERTopic
from sklearn.datasets import fetch_20newsgroups
import torch
from torch.utils.data import Dataset,DataLoader
import pandas as pd
import numpy as np
import torch.nn.functional as F


dataset = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))
data = dataset["data"]
label = dataset["target"]
label_names = dataset["target_names"]


data[0]


class News20Dataset(Dataset):
    def __init__(self,tokenizer=None,max_length=256,transforms=None):
        self.raw_dataset = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))
        self.data = self.raw_dataset["data"]
        self.label = self.raw_dataset["target"]
        self.label_name = self.raw_dataset["target_names"]
        self.tokenizer = tokenizer
        self.max_length = max_length
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        text = self.data[idx]
        encode_text = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors="pt")
        return encode_text,text,self.label[idx]
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


device = "cuda:0"
from transformers import AutoModel
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('dslim/bert-base-NER')
model = AutoModel.from_pretrained('dslim/bert-base-NER', trust_remote_code=True).to(device)


news20dataset = News20Dataset(tokenizer=tokenizer)


batch_size = 64
news20loader = DataLoader(news20dataset,batch_size=batch_size)
embeddings_all = []
with torch.no_grad():
    for batch in news20loader:
        encode_text,text,label = batch
        input_ids = encode_text["input_ids"].squeeze().to(device)
        attention_mask = encode_text["attention_mask"].squeeze().to(device)
        model_output = model(input_ids=input_ids, attention_mask=attention_mask)
        embeddings = mean_pooling(model_output, attention_mask)
        embeddings = F.normalize(embeddings, p=2, dim=1)
        # embeddings = torch.mean(model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state,dim=1).detach()
        embeddings_all.append(embeddings)



embeddings_all = torch.cat(embeddings_all,dim=0)


embeddings_all.shape


np.save('embeddings/bert-base-NER.npy', embeddings_all.cpu().numpy())


from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
tsne = TSNE(n_components=2, perplexity=5, learning_rate=500, n_iter=3000, early_exaggeration=15, random_state=42)
embeddings_2d = tsne.fit_transform(embeddings_all.cpu().numpy())


plt.figure(figsize=(8, 6))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],c=news20dataset.label,cmap='coolwarm')
plt.title("t-SNE of Model Embeddings")
plt.savefig("figures/tf-IDF_predict.png", dpi=300, bbox_inches='tight')
plt.show()



